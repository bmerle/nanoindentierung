{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dfd6a92-0949-49a1-a30e-fed4035a8f2f",
   "metadata": {},
   "source": [
    "# Unsupervised Machine Learning für die Nanoindentierung\n",
    "Übung zur **Vorlesung Nanoindentierung**\n",
    "\n",
    "Universität Kassel, 2026\n",
    "\n",
    "Skript und Daten von T. Arold, modfiziert durch B. Merle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723d9fc3-a52f-49ca-b93f-1931e37dd310",
   "metadata": {},
   "source": [
    "# Bereitstellen der Pakete und Funktionen\n",
    "Führen Sie den unten stehenden Code mit **strg + Enter** aus um alle notwendigen Zusatzpakete und Funktionen verfügbar zu machen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55440c4-20d4-4048-80fe-619d89417a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imported packages\n",
    "import pandas as pd # used to handle csv- or excel files as a dataframe (table object)\n",
    "import numpy as np # used for basic mathematical operations\n",
    "import matplotlib.pyplot as plt # package for basic data plotting\n",
    "from mpl_toolkits.mplot3d import Axes3D  # noqa: F401\n",
    "import seaborn as sns # additional package with more plotting options based on pyplot\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import os \n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "\n",
    "\n",
    "# from here code for more complexe plotting functions\n",
    "def _resolve_column(df: pd.DataFrame, col):\n",
    "    \"\"\"Resolve a column specifier for flat or MultiIndex columns.\n",
    "    - col can be None, a tuple (MultiIndex), or a string.\n",
    "    - Exact matches are preferred. If not found and df has MultiIndex,\n",
    "      try to find columns where any level equals the string.\n",
    "    \"\"\"\n",
    "   \n",
    "    if col is None:\n",
    "        return None, None  # (series, display_name)\n",
    "    # tuple (explicit MultiIndex)\n",
    "    if isinstance(col, tuple):\n",
    "        if col in df.columns:\n",
    "            return df[col], \" - \".join(map(str, col))\n",
    "        raise KeyError(f\"Column tuple {col} not found in DataFrame columns.\")\n",
    "    # string case\n",
    "    # direct exact match (flat columns)\n",
    "    if col in df.columns:\n",
    "        return df[col], str(col)\n",
    "    # if MultiIndex, try exact match on any level\n",
    "    if isinstance(df.columns, pd.MultiIndex):\n",
    "        # exact level match\n",
    "        matches = [c for c in df.columns if any(str(level) == col for level in c)]\n",
    "        if len(matches) == 1:\n",
    "            return df[matches[0]], \" - \".join(map(str, matches[0]))\n",
    "        if len(matches) > 1:\n",
    "            # prefer a match where top-level equals col\n",
    "            top_matches = [c for c in matches if str(c[0]) == col]\n",
    "            chosen = top_matches[0] if top_matches else matches[0]\n",
    "            print(f\"Multiple columns match '{col}', using {chosen}.\")\n",
    "            return df[chosen], \" - \".join(map(str, chosen))\n",
    "        # try substring match (e.g., user passes 'PC 1' and column is ('PCA','PC 1'))\n",
    "        substr_matches = [c for c in df.columns if any(col in str(level) for level in c)]\n",
    "        if len(substr_matches) >= 1:\n",
    "            chosen = substr_matches[0]\n",
    "            print(f\"No exact match for '{col}', using first substring match {chosen}.\")\n",
    "            return df[chosen], \" - \".join(map(str, chosen))\n",
    "    # fallback: try substring in flat columns\n",
    "    flat_sub = [c for c in df.columns if col in str(c)]\n",
    "    if len(flat_sub) >= 1:\n",
    "        chosen = flat_sub[0]\n",
    "        print(f\"No exact match for '{col}', using first flat substring match {chosen}.\")\n",
    "        return df[chosen], str(chosen)\n",
    "    raise KeyError(f\"Column '{col}' not found in DataFrame columns.\")\n",
    "\n",
    "def plot_dataframe(df: pd.DataFrame,\n",
    "                   x,\n",
    "                   y,\n",
    "                   z=None,\n",
    "                   hue=None,\n",
    "                   figsize=(9, 6),\n",
    "                   palette=None,\n",
    "                   point_size=40,\n",
    "                   alpha=0.9,\n",
    "                   cmap=\"viridis\",\n",
    "                   title=None):\n",
    "    \"\"\"\n",
    "    Flexible plotting for DataFrame: automatic 2D/3D scatter depending on z.\n",
    "    x, y, z, hue can be strings or tuples (for MultiIndex). If z is None -> 2D.\n",
    "    Returns (fig, ax).\n",
    "    \"\"\"\n",
    "    # Resolve columns and display names\n",
    "    X, x_label = _resolve_column(df, x)\n",
    "    Y, y_label = _resolve_column(df, y)\n",
    "    Z, z_label = _resolve_column(df, z) if z is not None else (None, None)\n",
    "    H, h_label = _resolve_column(df, hue) if hue is not None else (None, None)\n",
    "\n",
    "    if X is None or Y is None:\n",
    "        raise ValueError(\"x and y must be provided and resolvable to DataFrame columns.\")\n",
    "\n",
    "    x_vals = X.values\n",
    "    y_vals = Y.values\n",
    "    z_vals = Z.values if Z is not None else None\n",
    "    hue_vals = H.values if H is not None else None\n",
    "\n",
    "    is_3d = z_vals is not None\n",
    "\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    if is_3d:\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "    else:\n",
    "        ax = fig.add_subplot(111)\n",
    "\n",
    "    # No hue: single color\n",
    "    if hue_vals is None:\n",
    "        color = sns.color_palette()[0]\n",
    "        if is_3d:\n",
    "            sc = ax.scatter(x_vals, y_vals, z_vals, s=point_size, alpha=alpha, color=color)\n",
    "        else:\n",
    "            sc = ax.scatter(x_vals, y_vals, s=point_size, alpha=alpha, color=color)\n",
    "    else:\n",
    "        # Determine whether hue is numeric or categorical.\n",
    "        # If dtype is object or string-like -> categorical.\n",
    "        if pd.api.types.is_numeric_dtype(H):\n",
    "            # continuous hue\n",
    "            if is_3d:\n",
    "                # 3D scatter with continuous color: map to RGBA\n",
    "                norm = plt.Normalize(np.nanmin(hue_vals), np.nanmax(hue_vals))\n",
    "                cmap_obj = plt.get_cmap(cmap)\n",
    "                colors = cmap_obj(norm(hue_vals))\n",
    "                sc = ax.scatter(x_vals, y_vals, z_vals, c=colors, s=point_size, alpha=alpha)\n",
    "                # create a ScalarMappable for colorbar\n",
    "                mappable = plt.cm.ScalarMappable(norm=norm, cmap=cmap_obj)\n",
    "                mappable.set_array(hue_vals)\n",
    "                cbar = fig.colorbar(mappable, ax=ax, pad=0.1)\n",
    "                cbar.set_label(h_label or str(hue))\n",
    "            else:\n",
    "                sc = ax.scatter(x_vals, y_vals, c=hue_vals, cmap=cmap, s=point_size, alpha=alpha)\n",
    "                cbar = fig.colorbar(sc, ax=ax, pad=0.1)\n",
    "                cbar.set_label(h_label or str(hue))\n",
    "        else:\n",
    "            # categorical hue (strings or objects)\n",
    "            categories, uniques = pd.factorize(hue_vals)\n",
    "            n_cats = len(uniques)\n",
    "            if palette is None:\n",
    "                palette = sns.color_palette(n_colors=n_cats)\n",
    "            colors = [palette[i % len(palette)] for i in categories]\n",
    "            if is_3d:\n",
    "                sc = ax.scatter(x_vals, y_vals, z_vals, c=colors, s=point_size, alpha=alpha)\n",
    "            else:\n",
    "                sc = ax.scatter(x_vals, y_vals, c=colors, s=point_size, alpha=alpha)\n",
    "            # legend\n",
    "            handles = []\n",
    "            for i, lab in enumerate(uniques):\n",
    "                handles.append(plt.Line2D([], [], marker='o', color=palette[i % len(palette)], linestyle='', markersize=6))\n",
    "            ax.legend(handles, uniques, title=(h_label or str(hue)), bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "    # Labels and title\n",
    "    ax.set_xlabel(x_label or str(x))\n",
    "    ax.set_ylabel(y_label or str(y))\n",
    "    if is_3d:\n",
    "        ax.set_zlabel(z_label or str(z))\n",
    "    if title:\n",
    "        ax.set_title(title)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return fig, ax\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d59355-a048-42cd-b426-ade3de430e0b",
   "metadata": {},
   "source": [
    "# Importieren der Daten\n",
    "Importieren Sie mit dem unten stehenden Codeabschnitt _(Anpassungen des Codes sind nicht notwendig)_ die Daten aus dem CSV-Ordner in ein Pandas DataFrame. Nach der Ausführung wird das DataFrame als Tabelle unterhalb des Codeblocks angezeigt. Die Daten bestehen aus 14 Spalten. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269ac4b0-a68c-4745-aed5-68fd3c04351c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=== dont change paramter from here ===#\n",
    "# creating data frame from nanoindendation data in csv folder\n",
    "df = pd.read_csv(\"Data/CSV/OriginalData.csv\", # file path\n",
    "                header = [0,1], # number of rows for column heads\n",
    "                sep = \";\", # separator between columns\n",
    "                decimal = \",\" # decimal comma or decimal point\n",
    "                )\n",
    "\n",
    "# Set Indent column as Index\n",
    "df.index = df[(\"Unnamed: 0_level_0\",\"Unnamed: 0_level_1\")]\n",
    "df = df.drop((\"Unnamed: 0_level_0\",\"Unnamed: 0_level_1\"), axis=1)\n",
    "\n",
    "#visualization of the data structure\n",
    "\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856283d8-e989-431d-8f67-923ecd73cace",
   "metadata": {},
   "source": [
    "# Plotten der mechanischen Daten\n",
    "Mit der Ausführung des untenstehenden Codeblocks werden die mechanisch relevanten Daten als Mapping dargestellt. \n",
    "- Mit den `marker_options` und `colormap` können Sie die Darstellung des Plots beeinflussen. \n",
    "- Mit `indent_position = \"real\" oder \"ideal\"` legen Sie fest, ob die mechanischen Daten bezüglich ihrer Koordinaten aus der Rasterkraftmikroskopie oder der Daten aus der Nanoindentierung geplottet werden sollen.\n",
    "- Mit `data_left_plot` und `data_right_plot` können Sie festlegen welche Spalten des DataFrames geplottet werden sollen\n",
    "\n",
    "**Frage:**\n",
    "Können Sie beim Vergleich beider Plots erkennen welche Indents eine Gruppe bilden könnten?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a2bc77-e0bc-4c95-aeca-56fdba878c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === change parameter from here === #\n",
    "\n",
    "#choosing between ideal indent position and real indent positions\n",
    "indent_position = \"ideal\" # <-- \"real\" or \"ideal\"\n",
    "\n",
    "#marker options\n",
    "marker_transparency = 0.75 # setting the transparency of the markers\n",
    "\n",
    "#change the colormap of the plots\n",
    "colormap = \"crest_r\" #<-- \"viridis\", \"cividis\", \"inferno\" ,\"magma\", \"plasma\", \"rocket\", \"flare\", \"crest\", \"copper\"\n",
    "\n",
    "#selecting columns for plotting\n",
    "data_left_plot = (\"HARDNESS GPa\",\"mean\")\n",
    "data_right_plot = (\"MODULUS GPa\",\"mean\")\n",
    "# ========================= #\n",
    "\n",
    "#=== dont change paramter from here ===#\n",
    "\n",
    "#image size\n",
    "image_width_cm = 15 #change value to alter the image size\n",
    "image_height_cm = 15 # change value to alter the image size\n",
    "\n",
    "#Atomic Force Microscopy Mapping of Indentationmapping\n",
    "background_image = plt.imread(\"Data/Images/BackGround.png\")\n",
    "dx,dy = -2.5,-3.2 #parameter for adjusting the background image\n",
    "range_um = 50 # parameter for adjusting the background image\n",
    "\n",
    "# automatical column selection for indent position\n",
    "if indent_position == \"real\":\n",
    "    marker_shape = \">\"\n",
    "    marker_size = 40 # setting the size of the markers\n",
    "    x = (\"x\",\"real\") # defining x axis\n",
    "    y = (\"y\",\"real\") # defining y axis\n",
    "elif indent_position ==\"ideal\":\n",
    "    marker_shape = \"s\"\n",
    "    marker_size = 85 # setting the size of the markers\n",
    "    x = (\"x\",\"absolut\") # defining x axis\n",
    "    y = (\"y\",\"absolut\") # defining y axis\n",
    "\n",
    "#creating a figure object\n",
    "fig, ax = plt.subplots(nrows = 1,\n",
    "                       ncols = 2, #3 images next to each other\n",
    "                       sharey=True)\n",
    "\n",
    "\n",
    "fig.set_dpi(600) # increasing the resolution of the plot\n",
    "fig.set_size_inches(image_width_cm/2.54,image_height_cm/2.54) #calcuating image size\n",
    "\n",
    "# hardness plot \n",
    "sns.scatterplot(data = df,\n",
    "                x = x,\n",
    "                y = y,\n",
    "                hue = data_left_plot,\n",
    "                ax = ax[0], #left plot\n",
    "                marker = marker_shape, #square marker\n",
    "                palette = sns.color_palette(colormap,as_cmap = True), # defining the color palette\n",
    "                s = marker_size, # marker size\n",
    "                edgecolor = None, # remove the outline of the markers\n",
    "                alpha = marker_transparency , # transparency of the markers infill\n",
    "               )\n",
    "\n",
    "if indent_position == \"real\":\n",
    "    ax[0].imshow(background_image,\n",
    "                 extent=[dx,range_um+dx,dy,range_um+dy],\n",
    "                 aspect='equal',\n",
    "                 zorder=-1,\n",
    "                 origin = \"upper\"\n",
    "                )\n",
    "\n",
    "#modulus plot\n",
    "sns.scatterplot(data = df,\n",
    "                x = x,\n",
    "                y = y,\n",
    "                hue = data_right_plot,\n",
    "                ax = ax[1], #middle plot\n",
    "                marker = marker_shape, #square marker\n",
    "                palette = sns.color_palette(colormap,as_cmap = True), # defining the color palette\n",
    "                s = marker_size, # marker size\n",
    "                edgecolor = None, # remove the outline of the markers\n",
    "                alpha = marker_transparency , # transparency of the markers infill\n",
    "                )\n",
    "\n",
    "if indent_position == \"real\":\n",
    "    ax[1].imshow(background_image,\n",
    "                 extent=[dx,range_um+dx,dy,range_um+dy],\n",
    "                 aspect='equal',\n",
    "                 zorder=-1,\n",
    "                 origin = \"upper\"\n",
    "                )\n",
    "\n",
    "for item in ax:\n",
    "    item.set_aspect(\"equal\") # ensure equidistance on x and y axis\n",
    "    item.set_xlabel(\"X / µm\")\n",
    "    item.set_ylabel(\"Y / µm\")\n",
    "    sns.move_legend(loc = \"lower center\", bbox_to_anchor = (0.5,1), obj = item) # move legend above the corresponding plots\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797dc114-730b-40e4-b6e6-1a4b5210a5ea",
   "metadata": {},
   "source": [
    "# Clustering nach KMeans\n",
    "\n",
    "## Ausgangslage\n",
    "\n",
    "Im vorherigen Schritt haben Sie die Härte- und E-Modul-Maps visuell betrachtet. Dabei lassen sich bereits Bereiche mit unterschiedlichen mechanischen Eigenschaften möglicherweise erahnen — doch eine objektive, reproduzierbare Zuordnung einzelner Indents zu Werkstoffphasen ist allein per Augenmaß kaum möglich. Genau hier setzt **Clustering** an: ein Verfahren, das Datenpunkte automatisch in Gruppen einteilt, ohne dass die Gruppenzugehörigkeit vorher bekannt sein muss.\n",
    "\n",
    "## Was ist KMeans?\n",
    "\n",
    "KMeans ist einer der bekanntesten Clustering-Algorithmen und gehört zum sogenannten **unüberwachten Lernen** (*unsupervised learning*). „Unüberwacht\" bedeutet, dass dem Algorithmus keine gelabelten Trainingsdaten (z. B. „dieser Indent gehört zu Ferrit\") vorgegeben werden. Stattdessen sucht der Algorithmus selbstständig nach Strukturen in den Daten.\n",
    "\n",
    "Die Grundidee ist einfach: KMeans versucht, die Datenpunkte so in *k* Gruppen aufzuteilen, dass die **Summe der quadratischen Abstände** jedes Punktes zu seinem jeweiligen Clusterzentrum minimal wird. Das Clusterzentrum (auch *Zentroid* genannt) ist dabei der Mittelwert aller Punkte innerhalb eines Clusters.\n",
    "\n",
    "## Ablauf des Algorithmus\n",
    "\n",
    "1. **Initialisierung:** Es werden *k* zufällige Startpositionen für die Clusterzentren gewählt.\n",
    "2. **Zuweisungsschritt:** Jeder Datenpunkt wird dem Clusterzentrum zugeordnet, das ihm am nächsten liegt (euklidischer Abstand).\n",
    "3. **Aktualisierungsschritt:** Die Clusterzentren werden als Mittelwert aller zugeordneten Punkte neu berechnet.\n",
    "4. **Iteration:** Die Schritte 2 und 3 wiederholen sich, bis sich die Zuordnungen nicht mehr ändern oder ein Abbruchkriterium erreicht ist.\n",
    "\n",
    "Da KMeans auf euklidischen Abständen basiert, erzeugt der Algorithmus tendenziell **kugelförmige Cluster gleicher Größe**. Das ist eine wichtige Einschränkung, auf die wir später beim Vergleich mit dem Gaussian Mixture Model zurückkommen.\n",
    "\n",
    "## Parameter\n",
    "\n",
    "- `n_clusters`: Die Anzahl der Cluster *k*. Dieser Wert muss **vor** der Ausführung festgelegt werden. Wie man eine geeignete Anzahl bestimmt, wird in einem späteren Abschnitt behandelt.\n",
    "- `feature_set`: Welche Spalten des DataFrames als Merkmale (Features) für die Gruppierung verwendet werden.\n",
    "\n",
    "## Hinweis zur Auswahl des Feature-Sets\n",
    "\n",
    "Die Qualität des Clusterings hängt stark davon ab, welche Merkmale übergeben werden. Dabei gibt es zwei typische Fehlerquellen:\n",
    "\n",
    "- **Korrelierte Merkmale:** Härte in HV und Härte in GPa tragen dieselbe physikalische Information. Werden beide übergeben, wird diese Information doppelt gewichtet und verzerrt die Abstandsberechnung.\n",
    "- **Nicht-physikalische Merkmale:** Die x- und y-Koordinaten der Indents beschreiben die räumliche Position auf der Probe, nicht aber die Materialeigenschaften. Werden sie dem Algorithmus übergeben, bildet er Cluster teilweise nach Ort statt nach Phase.\n",
    "\n",
    "Es ist daher ratsam, dem KMeans-Algorithmus nur für die Fragestellung „Welche Phasen liegen vor?\" relevante Merkmale zu übergeben. Nach der Ausführung des Codes wird dem DataFrame eine neue Spalte `(\"KMeans\", \"Label\")` hinzugefügt, die für jeden Indent die Clusterzugehörigkeit enthält.\n",
    "\n",
    "**Nutzen Sie für die Übung zunächst `n_clusters = 4` oder höher.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da53f17d-3d9f-4136-9f6b-574102336951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === change parameter from here ===\n",
    "n_clusters = 4  # number of clusters\n",
    "feature_set = df[[\n",
    "                    (\"HARDNESS GPa\",\"mean\"),\n",
    "                    (\"MODULUS GPa\",\"mean\"),\n",
    "                    (\"x\",\"real\"),\n",
    "                    (\"y\",\"real\")\n",
    "                ]].copy()\n",
    "#=========================#\n",
    "\n",
    "#=== dont change paramter from here ===#\n",
    "\n",
    "\n",
    "# using the feature set defined \n",
    "X_kmeans = feature_set.copy().dropna()\n",
    "\n",
    "# KMeans calculations \n",
    "kmeans = KMeans(n_clusters=n_clusters, n_init='auto', random_state=42)\n",
    "labels = kmeans.fit_predict(X_kmeans).astype(int)\n",
    "\n",
    "# using strings for labelings instead of numbers\n",
    "#label_names = [f\"Cluster {i+1}\" for i in labels]\n",
    "labels = pd.DataFrame(labels, index=X_kmeans.index)\n",
    "\n",
    "# creating MultiIndex colum name for joining labels_df with df\n",
    "#labels_df.columns = pd.MultiIndex.from_tuples([('KMeans', 'Label')])\n",
    "labels.columns = pd.MultiIndex.from_tuples([('KMeans', 'Label')])\n",
    "# deleting KMeans grouping if it already exists\n",
    "if ('KMeans', 'Label') in df.columns:\n",
    "    df = df.drop(columns=('KMeans', 'Label'))\n",
    "\n",
    "# join KMeans Label_DataFrame with the original Dataframe df\n",
    "df = df.join(labels)\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf721d37-c72b-4057-9ee3-9ed266fec954",
   "metadata": {},
   "source": [
    "# Darstellen der KMeans-Ergebnisse\n",
    "\n",
    "Der folgende Codeblock visualisiert die Clustering-Ergebnisse auf zwei Arten:\n",
    "\n",
    "1. **Mapping:** Jeder Indent wird an seiner Position auf der Probe dargestellt und entsprechend seiner Clusterzugehörigkeit eingefärbt. So lässt sich erkennen, ob die gefundenen Cluster räumlich sinnvollen Bereichen (z. B. Phasen) entsprechen.\n",
    "2. **Violinplot:** Für jedes Cluster werden die Verteilungen von Härte, E-Modul und $S^2/P$ dargestellt. Damit lässt sich beurteilen, ob die Cluster tatsächlich unterschiedliche mechanische Eigenschaften aufweisen oder sich stark überlappen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331a1650-4cb2-47f4-8b92-889eb9c1ef3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=== change parameter from here ===#\n",
    "#marker options\n",
    "marker_shape = \">\" #<-- \"s\" for squares, \">\" for triangles\n",
    "marker_size = 85 # setting the size of the markers\n",
    "marker_transparency = 0.75 # setting the transparency of the markers\n",
    "\n",
    "#choosing between ideal indent position and real indent positions\n",
    "indent_position = \"real\" # <-- \"real\" or \"ideal\"\n",
    "\n",
    "#change the colormap of the plots\n",
    "colormap = \"tab10\" #<-- \"tab10\", \"tab20\", \"colorblind\" \n",
    "\n",
    "#=========================#\n",
    "\n",
    "#=== dont change paramter from here ===#\n",
    "n_cluster_kmeans = df[(\"KMeans\",\"Label\")].nunique()+2\n",
    "#image size\n",
    "image_width_cm = 15 #change value to alter the image size\n",
    "image_height_cm = 15 # change value to alter the image size\n",
    "\n",
    "#Atomic Force Microscopy Mapping of Indentationmapping\n",
    "background_image = plt.imread(\"Data/Images/BackGround.png\")\n",
    "dx,dy = -2,-3.2 #parameter for adjusting the background image\n",
    "range_um = 50 # parameter for adjusting the background image\n",
    "\n",
    "# automatical column selection for indent position\n",
    "if indent_position == \"real\":\n",
    "    x = (\"x\",\"real\") # defining x axis\n",
    "    y = (\"y\",\"real\") # defining y axis\n",
    "elif indent_position ==\"ideal\":\n",
    "    x = (\"x\",\"absolut\") # defining x axis\n",
    "    y = (\"y\",\"absolut\") # defining x axis\n",
    "\n",
    "#creating a figure object\n",
    "fig, ax = plt.subplots(nrows = 1,\n",
    "                       ncols = 1, \n",
    "                       sharey=False)\n",
    "\n",
    "\n",
    "fig.set_dpi(600) # increasing the resolution of the plot\n",
    "fig.set_size_inches(image_width_cm/2.54,image_height_cm/2.54) #calcuating image size\n",
    "\n",
    "# Code for Mapping\n",
    "sns.scatterplot(data = df,\n",
    "                x = x,\n",
    "                y = y,\n",
    "                hue = (\"KMeans\",\"Label\"),\n",
    "                ax = ax,\n",
    "                marker = marker_shape, #square marker\n",
    "                palette = sns.color_palette(colormap,n_cluster_kmeans)[2:], # defining the color palette\n",
    "                s = marker_size, # marker size\n",
    "                edgecolor = None, # remove the outline of the markers\n",
    "                alpha = marker_transparency , # transparency of the markers infill\n",
    "               )\n",
    "\n",
    "ax.grid(False)\n",
    "if indent_position == \"real\":\n",
    "    ax.imshow(background_image,\n",
    "                 extent=[dx,range_um+dx,dy,range_um+dy],\n",
    "                 aspect='equal',\n",
    "                 zorder=-1,\n",
    "                 origin = \"upper\"\n",
    "                )\n",
    "\n",
    "ax.set_aspect(\"equal\") # ensure equidistance on x and y axis\n",
    "ax.set_xlabel(\"X / µm\")\n",
    "ax.set_ylabel(\"Y / µm\")\n",
    "\n",
    "sns.move_legend(loc = \"lower center\", bbox_to_anchor = (0.5,1), obj = ax) # move legend above the corresponding plots\n",
    "\n",
    "#Code for Catplot\n",
    "\n",
    "cols = [\n",
    "    (\"HARDNESS GPa\", \"mean\"),\n",
    "    (\"MODULUS GPa\", \"mean\"),\n",
    "    (\"S2overP\", \"mean\"),\n",
    "    (\"KMeans\", \"Label\"),\n",
    "]\n",
    "\n",
    "df_sel = df[cols].copy()\n",
    "df_sel.columns = [\"Hardness\", \"Modulus\", \"S2overP\", \"Cluster\"]\n",
    "\n",
    "df_long = df_sel.melt(\n",
    "    id_vars=\"Cluster\",\n",
    "    var_name=\"Property\",\n",
    "    value_name=\"Value\"\n",
    ")\n",
    "\n",
    "g = sns.catplot(\n",
    "        data=df_long,\n",
    "        x=\"Cluster\",\n",
    "        y=\"Value\",\n",
    "        col=\"Property\",      # drei Plots nebeneinander\n",
    "        kind=\"violin\",          # Mittelwerte als Balken\n",
    "        sharey=False,\n",
    "        hue = \"Cluster\", # unterschiedliche Skalen erlaubt\n",
    "        palette = sns.color_palette(colormap,n_cluster_kmeans)[2:]\n",
    "        )\n",
    "g.figure.set_dpi(600)\n",
    "g.figure.set_size_inches(20/2.5,7/2.54)\n",
    "g._legend.remove()\n",
    "\n",
    "for ax in g.axes.flat:\n",
    "    ax.set_axisbelow(True)     \n",
    "    ax.grid(True, linestyle=\"--\", alpha=1,linewidth = 2)\n",
    "    ax.tick_params(axis=\"x\", rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d714830-e4ae-47a1-b0d7-5210eff829bd",
   "metadata": {},
   "source": [
    "## Bewertung der Ergebnisse\n",
    "\n",
    "Betrachten Sie die Violinplots der einzelnen Cluster. Es sollte auffallen, dass sich die mechanischen Eigenschaften der Gruppen **kaum voneinander unterscheiden** — die Verteilungen überlappen stark. Das deutet darauf hin, dass das Clustering in dieser Form keine sinnvolle Phasentrennung liefert.\n",
    "\n",
    "Mögliche Ursachen sind:\n",
    "\n",
    "- **Ungeeignetes Feature-Set:** Wurden z. B. korrelierte oder nicht-physikalische Merkmale mit übergeben?\n",
    "- **Falsche Clusteranzahl:** Stimmt *k* nicht mit der tatsächlichen Phasenanzahl überein?\n",
    "- **Zu hohe Dimensionalität:** Arbeitet der Algorithmus in zu vielen Dimensionen, was die Abstandsberechnung erschwert?\n",
    "\n",
    "In den nächsten Schritten werden wir diese Probleme systematisch angehen: zuerst mit einer **Dimensionsreduktion (PCA)**, dann mit einer **datengestützten Bestimmung der Clusteranzahl**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d64f71-73d7-4464-a57a-8744e4fe473d",
   "metadata": {},
   "source": [
    "# Principal Component Analysis (PCA)\n",
    "\n",
    "## Motivation\n",
    "\n",
    "Im vorherigen Abschnitt wurde der KMeans-Algorithmus direkt auf die Rohdaten angewendet. Dabei sind zwei Probleme aufgetreten:\n",
    "\n",
    "1. **Zu viele oder ungeeignete Merkmale:** Jede Spalte im Feature-Set entspricht einer Dimension im Datenraum. Werden z. B. fünf Spalten übergeben, arbeitet der Algorithmus in einem fünfdimensionalen Raum. Das erschwert nicht nur die Visualisierung (ab vier Dimensionen nicht mehr darstellbar), sondern kann auch die Clusterbildung verschlechtern — insbesondere wenn Merkmale stark korreliert oder für die Fragestellung irrelevant sind.\n",
    "\n",
    "2. **Korrelierte Merkmale verzerren Abstände:** Härte in GPa und Härte in HV tragen im Grunde dieselbe physikalische Information. Werden beide Spalten übergeben, wird diese Information doppelt gewichtet, was die Abstandsberechnung im Clusteralgorithmus verzerrt.\n",
    "\n",
    "Die **Principal Component Analysis** (PCA) löst beide Probleme gleichzeitig.\n",
    "\n",
    "## Was macht die PCA?\n",
    "\n",
    "Die PCA ist ein Verfahren aus der linearen Algebra, das einen hochdimensionalen Datensatz auf ein neues, orthogonales Koordinatensystem transformiert. Die neuen Achsen — die sogenannten **Hauptkomponenten** (Principal Components, PC) — werden so gewählt, dass:\n",
    "\n",
    "- **PC1** die Richtung im Datenraum beschreibt, entlang derer die Varianz (also die Streuung) der Daten am größten ist,\n",
    "- **PC2** senkrecht auf PC1 steht und die zweitgrößte Varianz erfasst,\n",
    "- und so weiter für jede weitere Komponente.\n",
    "\n",
    "Anschaulich kann man sich das wie folgt vorstellen: Stellen Sie sich eine Punktwolke in 3D vor — z. B. Messwerte aus der Nanoindentierung mit Härte, E-Modul und $S^2/P$ als Achsen. Die PCA legt zunächst eine neue Achse (PC1) durch die Richtung der größten Streuung dieser Wolke. PC2 steht senkrecht dazu und erfasst die verbliebene Streuung. Wenn die Punktwolke eher flach ist (wie eine Scheibe), reichen zwei Hauptkomponenten bereits aus, um die wesentliche Struktur der Daten zu beschreiben — die dritte Dimension kann verworfen werden, ohne relevante Information zu verlieren.\n",
    "\n",
    "## Warum ist das nützlich?\n",
    "\n",
    "Die PCA liefert zwei zentrale Ergebnisse:\n",
    "\n",
    "**1. Erklärte Varianz (Explained Variance Ratio)**\n",
    "Für jede Hauptkomponente wird angegeben, welcher Anteil der Gesamtstreuung durch diese Achse erklärt wird. Erklären z. B. PC1 und PC2 zusammen bereits 99 % der Varianz, dann steckt nahezu die gesamte Information des ursprünglichen Datensatzes in nur zwei Dimensionen. Die übrigen Komponenten können ohne nennenswerten Informationsverlust weggelassen werden. Das Ergebnis ist ein **dimensionsreduziertes Feature-Set**, das für den Clusteralgorithmus einfacher und robuster zu verarbeiten ist.\n",
    "\n",
    "**2. Ladungen (Loadings)**\n",
    "Die Loadings-Tabelle zeigt, wie stark jedes ursprüngliche Merkmal zu einer Hauptkomponente beiträgt. Ein hoher Absolutwert bedeutet eine starke Beteiligung, ein Wert nahe null bedeutet, dass dieses Merkmal für die jeweilige Komponente kaum relevant ist. Merkmale, die auf allen Komponenten nahe null liegen, tragen wenig zur Datenstruktur bei und können aus dem Feature-Set entfernt werden.\n",
    "\n",
    "## Wichtig: Standardisierung\n",
    "\n",
    "Vor der PCA werden die Daten **standardisiert** (z-Transformation): Jede Spalte wird so skaliert, dass sie einen Mittelwert von 0 und eine Standardabweichung von 1 besitzt. Das ist notwendig, weil die PCA auf Varianzen basiert. Ohne Standardisierung würde ein Merkmal mit großem Wertebereich (z. B. E-Modul in GPa) die Analyse dominieren, obwohl es nicht unbedingt informativer ist als ein Merkmal mit kleinerem Wertebereich (z. B. $S^2/P$). Die Standardisierung wird im Code automatisch mit `StandardScaler()` durchgeführt.\n",
    "\n",
    "## Hinweise zur Anwendung\n",
    "\n",
    "- **Nicht-physikalische Größen ausschließen:** Die x- und y-Koordinaten der Indents sind keine Materialkennwerte. Werden sie der PCA übergeben, fließt die räumliche Position in die Hauptkomponenten ein und verfälscht die Phasentrennung. Schließen Sie diese Spalten daher aus.\n",
    "- Physikalisch sinnvolle Merkmale zur Phasentrennung sind z. B.: Härte (GPa), E-Modul (GPa) und die quadrierte Steifigkeit bezogen auf die Last ($S^2/P$).\n",
    "\n",
    "**Parameter:**\n",
    "- Mit `features` definieren Sie, welche Spalten an die PCA übergeben werden.\n",
    "- Mit `n_components` legen Sie die Anzahl der Hauptkomponenten fest (belassen Sie den Wert zunächst auf 2).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc14dd33-fb98-4731-95cb-59a336c741cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=== change parameter ===#\n",
    "#selecting important columns for clustering\n",
    "features = df[[\n",
    "                (\"HARDNESS GPa\",\"mean\"),\n",
    "                (\"MODULUS GPa\",\"mean\"),\n",
    "                (\"S2overP\",\"mean\"),\n",
    "#            \t(\"x\",\"real\"),\n",
    "#                (\"y\",\"real\")\n",
    "                ]].copy()\n",
    "\n",
    "n_components = 2 #<-- determination of the dimension of the data space\n",
    "#=========================#\n",
    "\n",
    "#=== don't change parameters from here ===#\n",
    "\n",
    "#deleting indents with not data (Indent 15, Indent 67) \n",
    "features.dropna(inplace = True)\n",
    "\n",
    "# scale data very important for correct results!\n",
    "scaler = StandardScaler() \n",
    "X_scaled = scaler.fit_transform(features)\n",
    "\n",
    "# PCA on scaled data\n",
    "pca = PCA(n_components=n_components) \n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "fig = plt.figure(figsize=(8, 6)) \n",
    "if X_pca.shape[1] == 3:\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.scatter(X_pca[:, 0], X_pca[:, 1], X_pca[:, 2], c='steelblue', s=40)\n",
    "    ax.set_xlabel('PC1')\n",
    "    ax.set_ylabel('PC2')\n",
    "    ax.set_zlabel('PC3')\n",
    "    ax.set_title('PCA Scatterplot (3D)')\n",
    "    plt.show()\n",
    "    print(\"Erklärte Varianzanteile:\", pca.explained_variance_ratio_,\"\\n\")\n",
    "    loadings = pd.DataFrame(pca.components_.T, columns=['PCA1', 'PCA2',\"PCA3\"], index=features.columns ) \n",
    "    print(loadings)\n",
    "\n",
    "else:\n",
    "    plt.scatter(X_pca[:, 0], X_pca[:, 1], c='steelblue', s=40)\n",
    "    plt.xlabel('PC1')\n",
    "    plt.ylabel('PC2')\n",
    "    plt.title('PCA Scatterplot (2D)')\n",
    "    plt.show()\n",
    "    print(\"Erklärte Varianzanteile:\", pca.explained_variance_ratio_, \"\\n\")\n",
    "    loadings = pd.DataFrame(pca.components_.T, columns=['PC 1', 'PC 2'], index=features.columns ) \n",
    "    print(loadings)\n",
    "\n",
    "# deleting PCA columns in original data dataframe if present \n",
    "if 'PCA' in df.columns.get_level_values(0):\n",
    "    df = df.drop(columns='PCA', level=0)\n",
    "\n",
    "# make dataframe for PCA1 and PCA2 and maybe PCA3 axis\n",
    "pca_columns = [f'PC{i+1}' for i in range(X_pca.shape[1])]\n",
    "pca_df = pd.DataFrame(X_pca, columns=pca_columns, index=features.index)\n",
    "\n",
    "# transform to multi index column (\"PCA\",\"PC 1),(\"PC\",\"PC 2\"),...\n",
    "pca_df.columns = pd.MultiIndex.from_product([['PCA'], pca_columns])\n",
    "\n",
    "# join pca_df with orignal DataFrame df by Index \"Indent Nr\"\n",
    "df = df.join(pca_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab17dcf4-80b1-40d4-823d-2ba11e7c5645",
   "metadata": {},
   "source": [
    "## Interpretation der PCA-Ergebnisse\n",
    "\n",
    "**Erklärte Varianzanteile:** Die ausgegebenen Werte zeigen, welcher Anteil der Gesamtvarianz durch jede Hauptkomponente erklärt wird. Beschreiben PC1 und PC2 zusammen bereits ca. 99 % der Varianz, ist die Reduktion auf zwei Dimensionen gerechtfertigt — eine Erhöhung von `n_components` auf 3 würde keinen nennenswerten Informationsgewinn bringen und die Dimensionalität unnötig erhöhen.\n",
    "\n",
    "**Loadings-Tabelle:** Die Tabelle zeigt die Gewichtung jedes Merkmals auf den Hauptkomponenten. Beispielhaft:\n",
    "\n",
    "- Auf **PC1** besitzen Härte und E-Modul hohe Absolutwerte → PC1 trennt vor allem nach diesen mechanischen Eigenschaften.\n",
    "- Auf **PC2** besitzen E-Modul und $S^2/P$ hohe Absolutwerte → PC2 erfasst eine zusätzliche Variation, die durch PC1 allein nicht abgebildet wird.\n",
    "\n",
    "Merkmale mit Werten nahe null auf allen Komponenten tragen wenig zur Datenstruktur bei und können aus dem Feature-Set entfernt werden.\n",
    "\n",
    "**Hinweis:**\n",
    "Sollten die Ergebnisse nicht zufriedenstellend sein (z. B. unnötige Komponenten oder irrelevante Merkmale im Feature-Set), können Sie `features` und `n_components` anpassen und den Codeblock erneut ausführen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832e4faf-30dd-4f9e-b1c1-8e25569e57ff",
   "metadata": {},
   "source": [
    "# Bestimmen einer geeigneten Clusteranzahl\n",
    "\n",
    "Unüberwachte Clustering-Algorithmen wie KMeans oder das Gaussian Mixture Model benötigen die Angabe, in wie viele Cluster die Daten aufgeteilt werden sollen. In der Praxis ist diese Anzahl jedoch oft nicht bekannt — insbesondere wenn man die Daten explorativ nach Mustern durchsucht, etwa um unbekannte Phasen in einer Werkstoffprobe zu identifizieren.\n",
    "\n",
    "Es gibt daher quantitative Kriterien, die bei der Wahl einer geeigneten Clusteranzahl helfen. Der folgende Codeblock führt den KMeans-Algorithmus wiederholt für Clusteranzahlen von *k* = 2 bis *k* = 10 aus und wertet zwei solche Kriterien aus: den **Silhouette Score** und die **Inertia** (Elbow-Plot).\n",
    "\n",
    "## Silhouette Score\n",
    "\n",
    "Der Silhouette Score bewertet für jeden einzelnen Datenpunkt, wie gut er zu seinem eigenen Cluster passt im Vergleich zum nächstgelegenen fremden Cluster. Formal berechnet er sich aus zwei Größen:\n",
    "\n",
    "- **a**: der mittlere Abstand des Punktes zu allen anderen Punkten im selben Cluster (Maß für die Kompaktheit).\n",
    "- **b**: der mittlere Abstand des Punktes zu allen Punkten im nächstgelegenen fremden Cluster (Maß für die Trennung).\n",
    "\n",
    "Daraus ergibt sich:  $s = \\frac{b - a}{\\max(a, b)}$\n",
    "\n",
    "Der Wert liegt zwischen −1 und +1:\n",
    "\n",
    "- **Nahe +1:** Der Punkt liegt klar innerhalb seines Clusters und weit von anderen Clustern entfernt → gute Trennung.\n",
    "- **Nahe 0:** Der Punkt liegt an der Grenze zwischen zwei Clustern → uneindeutige Zuordnung.\n",
    "- **Negativ:** Der Punkt ist wahrscheinlich dem falschen Cluster zugeordnet.\n",
    "\n",
    "Der über alle Punkte gemittelte Silhouette Score dient als Gesamtmaß für die Clusterqualität. Die Clusteranzahl mit dem **höchsten** Silhouette Score ist in der Regel eine gute Wahl.\n",
    "\n",
    "## Inertia und Elbow-Plot\n",
    "\n",
    "Die **Inertia** (auch *Within-Cluster Sum of Squares*, WCSS) ist die Summe der quadratischen Abstände aller Punkte zu ihrem jeweiligen Clusterzentrum:\n",
    "\n",
    "$$\\text{Inertia} = \\sum_{i=1}^{k} \\sum_{x \\in C_i} \\|x - \\mu_i\\|^2$$\n",
    "\n",
    "Niedrige Werte bedeuten kompakte Cluster. Allerdings sinkt die Inertia **zwangsläufig** mit steigender Clusteranzahl — im Extremfall (ein Cluster pro Punkt) ist sie null. Deshalb kann die Inertia nicht als absolutes Kriterium dienen.\n",
    "\n",
    "Stattdessen wird sie im **Elbow-Plot** gegen die Clusteranzahl aufgetragen. Gesucht wird ein „Knick\" (Ellbogen) in der Kurve: Ab diesem Punkt bringt ein zusätzliches Cluster nur noch eine geringe Reduktion der Inertia — der Informationsgewinn rechtfertigt die höhere Komplexität nicht mehr. In der Praxis ist der Knick allerdings nicht immer eindeutig erkennbar, weshalb der Elbow-Plot am besten in Kombination mit dem Silhouette Score interpretiert wird.\n",
    "\n",
    "**Hinweis:** Mit `feature_set` legen Sie fest, ob das dimensionsreduzierte PCA-Feature-Set oder das originale Feature-Set verwendet wird. Da bereits eine PCA durchgeführt wurde, empfiehlt es sich, `feature_set = \"PCA\"` zu wählen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2181b75a-b594-429e-a7d4-7ae857227709",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=== Change from here ===#\n",
    "feature_set = \"PCA\" #<-- \"PCA\" or \"features\"\n",
    "#========================#\n",
    "\n",
    "#=== Dont change from here ===#\n",
    "max_number_cluster = 10\n",
    "silhouette_scores = []\n",
    "inertias = []\n",
    "ks = []\n",
    "\n",
    "# Select data basis\n",
    "if feature_set == 'PCA':\n",
    "    if (\"PCA\",\"PC1\") not in df.columns:\n",
    "        raise ValueError(f\"Für 'feature_set = PCA' bitte zuvor die PCA-Analyse (Code:Durchführen PCA) ausführen!\")\n",
    "    X = df['PCA'].dropna()\n",
    "elif feature_set == 'features':\n",
    "    X = features.copy()\n",
    "else:\n",
    "    raise ValueError(\"features_source must be 'PCA' or 'features'.\")\n",
    "\n",
    "for k in range(2,max_number_cluster):\n",
    "    kmeans = KMeans(n_clusters=k, n_init='auto', random_state=42)\n",
    "    labels = kmeans.fit_predict(X)\n",
    "    inertia = kmeans.inertia_\n",
    "    inertias.append(inertia)\n",
    "    ks.append(k)\n",
    "\n",
    "# Silhouette Score is only defined for k > 1\n",
    "    score = silhouette_score(X, labels)\n",
    "    silhouette_scores.append(score)\n",
    "    print(f\"k = {k}, Inertia = {inertia:.2f}, Silhouette Score = {score:.3f}\")\n",
    "\n",
    "# Plotting\n",
    "fig, ax1 = plt.subplots(figsize=(9, 5))\n",
    "\n",
    "color1 = 'tab:blue'\n",
    "ax1.set_xlabel('Number of Clusters (k)')\n",
    "ax1.set_ylabel('Inertia (Elbow)', color=color1)\n",
    "ax1.plot(ks, inertias, marker='o', color=color1, label='Inertia')\n",
    "ax1.tick_params(axis='y', labelcolor=color1)\n",
    "\n",
    "ax2 = ax1.twinx()  # second y-axis for Silhouette Score\n",
    "color2 = 'tab:green'\n",
    "ax2.set_ylabel('Silhouette Score', color=color2)\n",
    "ax2.plot(ks, silhouette_scores, marker='s', linestyle='--', color=color2, label='Silhouette Score')\n",
    "ax2.tick_params(axis='y', labelcolor=color2)\n",
    "\n",
    "plt.title('KMeans: Elbow Plot & Silhouette Score')\n",
    "fig.tight_layout()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36baf90d-f9f8-41de-bf8b-d25f3cac9605",
   "metadata": {},
   "source": [
    "In dem obigen Plot sind der Elbow-Plot und der Silhouette Score überlagert in Abhängigkeit von der Cluster-Anzahl dargestellt.\n",
    "\n",
    "Beurteilen Sie anhand des Plots, welche Clustergröße für den KMeans-Algorithmus am besten zu verwenden ist."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c5190f-29f3-4155-a906-df79d313e0f0",
   "metadata": {},
   "source": [
    "# Optimiertes Clustering nach KMeans\n",
    "\n",
    "Im ersten KMeans-Durchlauf (oben) wurden die Rohdaten direkt verwendet, was zu schlecht getrennten Clustern führte. Inzwischen wurden zwei Verbesserungen vorgenommen:\n",
    "\n",
    "1. **Dimensionsreduktion durch PCA:** Das Feature-Set wurde auf die Hauptkomponenten PC1 und PC2 reduziert, die den Großteil der Varianz abbilden.\n",
    "2. **Datengestützte Wahl von *k*:** Silhouette Score und Elbow-Plot liefern eine fundierte Grundlage für die Clusteranzahl.\n",
    "\n",
    "Der folgende Codeblock führt das KMeans-Clustering auf dem PCA-Feature-Set durch. Passen Sie `n_clusters` anhand Ihrer Auswertung des Silhouette Scores und des Elbow-Plots an.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa71276-e7a5-481d-9209-8dc15db03083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === change parameter from here ===#\n",
    "# KMeans Clustersize\n",
    "n_clusters = 4  # number of clusters\n",
    "\n",
    "# Plot Options\n",
    "#marker options\n",
    "marker_shape = \">\" #<-- \"s\" for squares, \">\" for triangles\n",
    "marker_size = 85 # setting the size of the markers\n",
    "marker_transparency = 0.75 # setting the transparency of the markers\n",
    "\n",
    "#choosing between ideal indent position and real indent positions\n",
    "indent_position = \"real\" # <-- \"real\" or \"ideal\"\n",
    "\n",
    "#change the colormap of the plots\n",
    "colormap = \"tab10\" #<-- \"tab10\", \"tab20\", \"colorblind\" \n",
    "#=========================#\n",
    "\n",
    "#=== dont change paramter from here ===#\n",
    "\n",
    "#Code for KMeans\n",
    "feature_pca = df.xs(\"PCA\",axis=1,level=0).copy()\n",
    "# using the feature set defined \n",
    "X_kmeans = feature_pca.dropna()\n",
    "\n",
    "# KMeans calculations \n",
    "kmeans = KMeans(n_clusters=n_clusters, n_init='auto', random_state=42)\n",
    "labels = kmeans.fit_predict(X_kmeans).astype(int)\n",
    "\n",
    "# using strings for labelings instead of numbers\n",
    "#label_names = [f\"Cluster {i+1}\" for i in labels]\n",
    "labels = pd.DataFrame(labels, index=X_kmeans.index)\n",
    "\n",
    "# creating MultiIndex colum name for joining labels_df with df\n",
    "#labels_df.columns = pd.MultiIndex.from_tuples([('KMeans', 'Label')])\n",
    "labels.columns = pd.MultiIndex.from_tuples([('KMeans', 'Label')])\n",
    "# deleting KMeans grouping if it already exists\n",
    "if ('KMeans', 'Label') in df.columns:\n",
    "    df = df.drop(columns=('KMeans', 'Label'))\n",
    "\n",
    "# join KMeans Label_DataFrame with the original Dataframe df\n",
    "df = df.join(labels)\n",
    "\n",
    "#Code for plotting\n",
    "\n",
    "n_cluster_kmeans = df[(\"KMeans\",\"Label\")].nunique()+2\n",
    "\n",
    "#image size\n",
    "image_width_cm = 15 #change value to alter the image size\n",
    "image_height_cm = 15 # change value to alter the image size\n",
    "\n",
    "#Atomic Force Microscopy Mapping of Indentationmapping\n",
    "background_image = plt.imread(\"Data/Images/BackGround.png\")\n",
    "dx,dy = -2,-3.2 #parameter for adjusting the background image\n",
    "range_um = 50 # parameter for adjusting the background image\n",
    "\n",
    "# automatical column selection for indent position\n",
    "if indent_position == \"real\":\n",
    "    x = (\"x\",\"real\") # defining x axis\n",
    "    y = (\"y\",\"real\") # defining y axis\n",
    "elif indent_position ==\"ideal\":\n",
    "    x = (\"x\",\"absolut\") # defining x axis\n",
    "    y = (\"y\",\"absolut\") # defining y axis\n",
    "\n",
    "#creating a figure object\n",
    "fig_map_kmeans, ax = plt.subplots(nrows = 1,\n",
    "                       ncols = 1, \n",
    "                       sharey=False)\n",
    "\n",
    "\n",
    "fig_map_kmeans.set_dpi(600) # increasing the resolution of the plot\n",
    "fig_map_kmeans.set_size_inches(image_width_cm/2.54,image_height_cm/2.54) # calculating image size\n",
    "\n",
    "# Code for Mapping\n",
    "map_kmeans = sns.scatterplot(data = df,\n",
    "                x = x,\n",
    "                y = y,\n",
    "                hue = (\"KMeans\",\"Label\"),\n",
    "                ax = ax,\n",
    "                marker = marker_shape, #square marker\n",
    "                palette = sns.color_palette(colormap,n_cluster_kmeans)[2:], # defining the color palette\n",
    "                s = marker_size, # marker size\n",
    "                edgecolor = None, # remove the outline of the markers\n",
    "                alpha = marker_transparency , # transparency of the markers infill\n",
    "               )\n",
    "\n",
    "ax.grid(False)\n",
    "if indent_position == \"real\":\n",
    "    ax.imshow(background_image,\n",
    "                 extent=[dx,range_um+dx,dy,range_um+dy],\n",
    "                 aspect='equal',\n",
    "                 zorder=-1,\n",
    "                 origin = \"upper\"\n",
    "                )\n",
    "\n",
    "ax.set_aspect(\"equal\") # ensure equidistance on x and y axis\n",
    "sns.move_legend(loc = \"lower center\", bbox_to_anchor = (0.5,1), obj = ax) # move legend above the corresponding plots\n",
    "\n",
    "#Code for Catplot\n",
    "\n",
    "cols = [\n",
    "    (\"HARDNESS GPa\", \"mean\"),\n",
    "    (\"MODULUS GPa\", \"mean\"),\n",
    "    (\"S2overP\", \"mean\"),\n",
    "    (\"KMeans\", \"Label\"),\n",
    "]\n",
    "\n",
    "df_sel = df[cols].copy()\n",
    "df_sel.columns = [\"Hardness\", \"Modulus\", \"S2overP\", \"Cluster\"]\n",
    "\n",
    "df_long = df_sel.melt(\n",
    "    id_vars=\"Cluster\",\n",
    "    var_name=\"Property\",\n",
    "    value_name=\"Value\"\n",
    ")\n",
    "\n",
    "Cat_kmeans = sns.catplot(\n",
    "        data=df_long,\n",
    "        x=\"Cluster\",\n",
    "        y=\"Value\",\n",
    "        col=\"Property\",      # drei Plots nebeneinander\n",
    "        kind=\"violin\",          # Mittelwerte als Balken\n",
    "        sharey=False,\n",
    "        hue = \"Cluster\", # unterschiedliche Skalen erlaubt\n",
    "        palette = sns.color_palette(colormap,n_cluster_kmeans)[2:]\n",
    "        )\n",
    "Cat_kmeans.figure.set_dpi(600)\n",
    "Cat_kmeans.figure.set_size_inches(20/2.5,7/2.54)\n",
    "Cat_kmeans._legend.remove()\n",
    "\n",
    "for ax in Cat_kmeans.axes.flat:\n",
    "    ax.set_axisbelow(True)     \n",
    "    ax.grid(True, linestyle=\"--\", alpha=1,linewidth = 2)\n",
    "    ax.tick_params(axis=\"x\", rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630884a2-4a7b-44f0-905b-3cf2c290ca94",
   "metadata": {},
   "source": [
    "## Vergleich: Vorher vs. Nachher\n",
    "\n",
    "Vergleichen Sie das aktuelle Mapping und die Violinplots mit den Ergebnissen der unoptimierten KMeans-Analyse von weiter oben. Achten Sie insbesondere auf:\n",
    "\n",
    "- **Räumliche Kohärenz:** Bilden die Cluster jetzt zusammenhängende Bereiche auf der Probe?\n",
    "- **Mechanische Trennschärfe:** Überlappen die Verteilungen von Härte, E-Modul und $S^2/P$ zwischen den Clustern weniger als zuvor?\n",
    "\n",
    "Es sollte deutlich werden, dass die Kombination aus PCA und datengestützter Clusteranzahl zu einer erheblich besseren Phasentrennung führt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1a6040-9db6-4fc4-9f27-949794a2e015",
   "metadata": {},
   "source": [
    "# Clustering nach Gaussian Mixture Model (GMM)\n",
    "\n",
    "## Motivation\n",
    "\n",
    "KMeans minimiert quadratische Abstände zu Clusterzentren und erzeugt dadurch tendenziell **kugelförmige Cluster gleicher Größe**. In der Realität sind Phasenbereiche in Werkstoffproben jedoch nicht immer gleich groß oder symmetrisch verteilt. Das **Gaussian Mixture Model** (GMM) bietet hier eine flexiblere Alternative.\n",
    "\n",
    "## Was ist ein Gaussian Mixture Model?\n",
    "\n",
    "GMM gehört ebenfalls zum unüberwachten Lernen, verfolgt aber einen grundlegend anderen Ansatz als KMeans. Die Grundidee: Man nimmt an, dass die beobachteten Datenpunkte aus einer **Mischung mehrerer Normalverteilungen** stammen — jede Normalverteilung repräsentiert ein Cluster.\n",
    "\n",
    "Jede dieser Normalverteilungen wird durch drei Größen beschrieben:\n",
    "\n",
    "- **Mittelwertvektor $\\mu$:** das Zentrum des Clusters im Merkmalsraum.\n",
    "- **Kovarianzmatrix $\\Sigma$:** beschreibt die Form, Ausdehnung und Orientierung des Clusters. Im Gegensatz zu KMeans, das implizit von kugelförmigen Clustern ausgeht, kann GMM durch die freie Kovarianzmatrix auch **elliptische Cluster** mit beliebiger Orientierung modellieren.\n",
    "- **Mischungsgewicht $\\pi$:** der Anteil der Datenpunkte, der zu diesem Cluster gehört.\n",
    "\n",
    "## Ablauf des Algorithmus (EM-Algorithmus)\n",
    "\n",
    "GMM wird mit dem **Expectation-Maximization (EM)**-Algorithmus optimiert, einem iterativen Verfahren:\n",
    "\n",
    "1. **Initialisierung:** Für jedes Cluster werden Startwerte für $\\mu$, $\\Sigma$ und $\\pi$ festgelegt.\n",
    "2. **E-Schritt (Expectation):** Für jeden Datenpunkt wird die **Zugehörigkeitswahrscheinlichkeit** zu jedem Cluster berechnet. Ein Punkt kann also z. B. mit 80 % zu Cluster 1 und mit 20 % zu Cluster 2 gehören — im Gegensatz zu KMeans, das immer eine harte Zuordnung vornimmt.\n",
    "3. **M-Schritt (Maximization):** Die Parameter $\\mu$, $\\Sigma$ und $\\pi$ jedes Clusters werden so aktualisiert, dass die **Gesamtwahrscheinlichkeit (Likelihood)** der Daten maximal wird.\n",
    "4. **Iteration:** E- und M-Schritt wechseln sich ab, bis die Parameter konvergieren.\n",
    "5. **Clusterzuweisung:** Jedem Punkt wird das Cluster mit der höchsten Zugehörigkeitswahrscheinlichkeit zugewiesen.\n",
    "\n",
    "## Bestimmung der Clusteranzahl für GMM\n",
    "\n",
    "Da GMM und KMeans unterschiedliche Modelle verwenden, muss die geeignete Clusteranzahl **für jeden Algorithmus separat** bestimmt werden. Der folgende Codeblock wertet drei Kriterien aus:\n",
    "\n",
    "**Silhouette Score** — wie beim KMeans (siehe oben).\n",
    "\n",
    "**BIC (Bayesian Information Criterion) und AIC (Akaike Information Criterion):**\n",
    "BIC und AIC sind Informationskriterien, die die **Modellgüte** (wie gut das Modell die Daten erklärt) gegen die **Modellkomplexität** (Anzahl der Parameter) abwägen. Beide basieren auf der Log-Likelihood, bestrafen aber zusätzlich die Anzahl freier Parameter:\n",
    "\n",
    "- **AIC** bestraft die Komplexität moderat und tendiert daher manchmal zu Modellen mit mehr Clustern.\n",
    "- **BIC** bestraft die Komplexität stärker (abhängig von der Stichprobengröße) und bevorzugt daher sparsamere Modelle mit weniger Clustern.\n",
    "\n",
    "In beiden Fällen gilt: **kleiner ist besser**. Gesucht wird die Clusteranzahl, bei der BIC und/oder AIC ein Minimum erreichen — idealerweise in Übereinstimmung mit einem hohen Silhouette Score.\n",
    "\n",
    "**Hinweis:** Mit `feature_set` legen Sie fest, ob das PCA-Feature-Set oder das originale Feature-Set verwendet wird.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0e429b-9385-44a7-b674-ce6e0627629c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Change from here === #\n",
    "feature_set = \"PCA\" #<-- \"PCA\" or \"features\"\n",
    "# ======================== #\n",
    "\n",
    "silhouette_scores = []\n",
    "neg_log_likelihoods = []\n",
    "bic_scores = []\n",
    "aic_scores = []\n",
    "ks = []\n",
    "\n",
    "# Select data basis\n",
    "if feature_set == 'PCA':\n",
    "    X = df['PCA'].dropna()\n",
    "elif feature_set == 'features':\n",
    "    X = features.copy()\n",
    "else:\n",
    "    raise ValueError(\"features_source must be 'PCA' or 'features'.\")\n",
    "\n",
    "# Compute metrics for different k\n",
    "for k in range(2, max_number_cluster):\n",
    "    gmm = GaussianMixture(n_components=k, random_state=42, n_init=5)\n",
    "    gmm.fit(X)\n",
    "    labels = gmm.predict(X)\n",
    "    \n",
    "    neg_ll = -gmm.score(X) * len(X)\n",
    "    bic = gmm.bic(X)\n",
    "    aic = gmm.aic(X)\n",
    "    score = silhouette_score(X, labels)\n",
    "    \n",
    "    neg_log_likelihoods.append(neg_ll)\n",
    "    bic_scores.append(bic)\n",
    "    aic_scores.append(aic)\n",
    "    silhouette_scores.append(score)\n",
    "    ks.append(k)\n",
    "\n",
    "# === Plot 1: -Log-Likelihood + Silhouette ===\n",
    "fig, ax1 = plt.subplots(figsize=(9,5))\n",
    "\n",
    "color1 = 'tab:purple'\n",
    "ax1.set_xlabel('Number of Clusters (k)')\n",
    "ax1.set_ylabel('− Log-Likelihood', color=color1)\n",
    "ax1.plot(ks, neg_log_likelihoods, marker='o', color=color1, label='− Log-Likelihood')\n",
    "ax1.tick_params(axis='y', labelcolor=color1)\n",
    "ax1.grid(True)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "color2 = 'tab:green'\n",
    "ax2.set_ylabel('Silhouette Score', color=color2)\n",
    "ax2.plot(ks, silhouette_scores, marker='s', linestyle='--', color=color2, label='Silhouette Score')\n",
    "ax2.tick_params(axis='y', labelcolor=color2)\n",
    "\n",
    "# Combined legend\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper right')\n",
    "\n",
    "plt.title('GMM: −Log-Likelihood & Silhouette Score')\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# === Plot 2: BIC + AIC ===\n",
    "plt.figure(figsize=(9,5))\n",
    "plt.plot(ks, bic_scores, marker='x', linestyle='--', color='red', label='BIC')\n",
    "plt.plot(ks, aic_scores, marker='^', linestyle='--', color='orange', label='AIC')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Information Criteria')\n",
    "plt.title('GMM: BIC & AIC')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf24a43-bdbe-47a2-a773-be4552985644",
   "metadata": {},
   "source": [
    "## Interpretation\n",
    "\n",
    "Betrachten Sie die Kriterien-Plots (Silhouette Score, BIC, AIC) und bestimmen Sie die geeignete Clusteranzahl für den GMM-Algorithmus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa0e52e-77b3-46c1-9351-7b7607601276",
   "metadata": {},
   "source": [
    "## Vergleich KMeans vs Gaussian Mixture Model\n",
    "\n",
    "| Eigenschaft | KMeans | GMM |\n",
    "|---|---|---|\n",
    "| Clusterform | kugelförmig, gleich groß | beliebig elliptisch |\n",
    "| Optimiert | Summe quadratischer Abstände | Likelihood |\n",
    "| Empfindlich auf | Ausreißer, ungleiche Clustergrößen | Überanpassung bei wenig Daten |\n",
    "\n",
    "**Hinweis:** Überprüfen Sie vor der Ausführung des nächsten Codeblocks, ob der Parameter `n_components` der gewählten Clusteranzahl entspricht, und passen Sie den Wert gegebenenfalls an.\n",
    "\n",
    "Die anschließend erzeugten Plots stellen die Ergebnisse von KMeans und GMM direkt gegenüber. Beurteilen Sie:\n",
    "\n",
    "- **Clusteranzahl:** Führen beide Algorithmen zur gleichen optimalen Anzahl an Clustern?\n",
    "- **Mechanische Eigenschaften:** Unterscheiden sich die ermittelten Härte-, E-Modul- und $S^2/P$-Verteilungen der Cluster zwischen den Algorithmen?\n",
    "- **Räumliche Verteilung:** Stimmen die Mappings beider Algorithmen überein, oder gibt es Bereiche, in denen sie unterschiedliche Zuordnungen liefern?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70587025-bf42-4e91-8179-9150638b0217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === change parameter from here ===#\n",
    "# GMM Cluster number\n",
    "n_components = 4  # number of clusters\n",
    "\n",
    "# Plot Options\n",
    "marker_shape = \">\"  # \"<\" for triangles, \"s\" for squares\n",
    "marker_size = 40\n",
    "marker_transparency = 0.75\n",
    "\n",
    "# choosing between ideal indent position and real indent positions\n",
    "indent_position = \"real\"  # \"real\" or \"ideal\"\n",
    "\n",
    "# colormap\n",
    "colormap = \"tab10\"  # \"tab10\", \"tab20\", \"colorblind\"\n",
    "#=========================#\n",
    "\n",
    "#=== don't change from here ===#\n",
    "\n",
    "# the allowed shape of the clusters\n",
    "covariance_type = \"full\" #<-- full, tied, diag, spherical\n",
    "\n",
    "# --- Prepare feature set ---\n",
    "feature_pca = df.xs(\"PCA\", axis=1, level=0).copy()\n",
    "X_gmm = feature_pca.dropna()\n",
    "\n",
    "# --- Fit GMM ---\n",
    "gmm = GaussianMixture(n_components=n_components, random_state=42, n_init=5,covariance_type = covariance_type)\n",
    "labels = gmm.fit_predict(X_gmm).astype(int)\n",
    "\n",
    "# convert to DataFrame and MultiIndex column for joining\n",
    "labels = pd.DataFrame(labels, index=X_gmm.index)\n",
    "labels.columns = pd.MultiIndex.from_tuples([('GMM', 'Label')])\n",
    "\n",
    "# remove existing GMM column if exists\n",
    "if ('GMM', 'Label') in df.columns:\n",
    "    df = df.drop(columns=('GMM', 'Label'))\n",
    "\n",
    "# join labels with original df\n",
    "df = df.join(labels)\n",
    "\n",
    "# --- Scatterplot / Mapping ---\n",
    "\n",
    "n_clusters_kmeans = df[(\"KMeans\", \"Label\")].nunique() + 2\n",
    "n_clusters_gmm = df[(\"GMM\",\"Label\")].nunique() + 2\n",
    "\n",
    "\n",
    "image_width_cm = 15\n",
    "image_height_cm = 15\n",
    "background_image = plt.imread(\"Data/Images/BackGround.png\")\n",
    "dx, dy = -2, -3.2\n",
    "range_um = 50\n",
    "\n",
    "if indent_position == \"real\":\n",
    "    x = (\"x\",\"real\")\n",
    "    y = (\"y\",\"real\")\n",
    "elif indent_position == \"ideal\":\n",
    "    x = (\"x\",\"absolut\")\n",
    "    y = (\"y\",\"absolut\")\n",
    "\n",
    "fig_map, ax = plt.subplots(ncols=2,\n",
    "                           nrows=1,\n",
    "                          sharey=True)\n",
    "fig_map.set_dpi(600)\n",
    "fig_map.set_size_inches(image_width_cm/2.5, image_height_cm/2.54)\n",
    "\n",
    "sns.scatterplot(\n",
    "    data=df,\n",
    "    x=x,\n",
    "    y=y,\n",
    "    hue=(\"KMeans\",\"Label\"),\n",
    "    ax=ax[0],\n",
    "    marker=marker_shape,\n",
    "    palette=sns.color_palette(colormap,n_clusters_kmeans)[2:],\n",
    "    s=marker_size,\n",
    "    edgecolor=None,\n",
    "    alpha=marker_transparency\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "if indent_position == \"real\":\n",
    "    ax[0].imshow(\n",
    "        background_image,\n",
    "        extent=[dx, range_um+dx, dy, range_um+dy],\n",
    "        aspect='equal',\n",
    "        zorder=-1,\n",
    "        origin=\"upper\"\n",
    "    )\n",
    "ax[0].set_axisbelow(True)\n",
    "ax[0].set_aspect(\"equal\")\n",
    "\n",
    "sns.scatterplot(\n",
    "    data=df,\n",
    "    x=x,\n",
    "    y=y,\n",
    "    hue=(\"GMM\",\"Label\"),\n",
    "    ax=ax[1],\n",
    "    marker=marker_shape,\n",
    "    palette=sns.color_palette(colormap,n_clusters_gmm)[2:],\n",
    "    s=marker_size,\n",
    "    edgecolor=None,\n",
    "    alpha=marker_transparency\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "if indent_position == \"real\":\n",
    "    ax[1].imshow(\n",
    "        background_image,\n",
    "        extent=[dx, range_um+dx, dy, range_um+dy],\n",
    "        aspect='equal',\n",
    "        zorder=-1,\n",
    "        origin=\"upper\"\n",
    "    )\n",
    "ax[1].set_axisbelow(True)\n",
    "ax[1].set_aspect(\"equal\")\n",
    "for item in ax:\n",
    "    sns.move_legend(loc=\"lower center\", bbox_to_anchor=(0.5,1), obj=item)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Spalten für Properties\n",
    "properties = [(\"HARDNESS GPa\",\"mean\"),(\"MODULUS GPa\",\"mean\"), (\"S2overP\",\"mean\")]\n",
    "n_props = len(properties)\n",
    "\n",
    "# Figure mit 2 Zeilen (KMeans, GMM) und n_props Spalten\n",
    "fig, axes = plt.subplots(nrows=2, ncols=n_props, figsize=(4*n_props, 8), sharey=False)\n",
    "\n",
    "# Farbpalette\n",
    "palette_kmeans = sns.color_palette(colormap,n_clusters_kmeans)[2:]\n",
    "palette_gmm = sns.color_palette(colormap,n_clusters_gmm)[2:]\n",
    "\n",
    "# --- Erste Zeile: KMeans ---\n",
    "for i, prop in enumerate(properties):\n",
    "\n",
    "    y = df[prop].dropna()\n",
    "    sns.violinplot(\n",
    "        data=df,\n",
    "        x=df[(\"KMeans\",\"Label\")],\n",
    "        y=y,\n",
    "        hue=df[(\"KMeans\",\"Label\")],\n",
    "        ax=axes[0, i],\n",
    "        palette=palette_kmeans,\n",
    "        split=False\n",
    "    )\n",
    "    axes[0, i].set_title(f\"{prop} (KMeans)\")\n",
    "    axes[0, i].tick_params(axis=\"x\", rotation=45)\n",
    "    axes[0, i].set_xlabel(\"\")\n",
    "    axes[0, i].grid(True, linestyle=\"--\", alpha=0.4)\n",
    "    axes[0, i].set_axisbelow(True)   \n",
    "    axes[0, i].legend_.remove()\n",
    "\n",
    "# --- Zweite Zeile: GMM ---\n",
    "for i, prop in enumerate(properties):\n",
    "    y = df[prop].dropna()\n",
    "    sns.violinplot(\n",
    "        data=df,\n",
    "        x=df[(\"GMM\",\"Label\")],\n",
    "        y=y,\n",
    "        hue=df[(\"GMM\",\"Label\")],\n",
    "        ax=axes[1, i],\n",
    "        palette=palette_gmm,\n",
    "        split=False\n",
    "    )\n",
    "    axes[1, i].set_title(f\"{prop} (GMM)\")\n",
    "    axes[1, i].tick_params(axis=\"x\", rotation=45)\n",
    "    axes[1, i].set_xlabel(\"\")\n",
    "    axes[1, i].grid(True, linestyle=\"--\", alpha=0.4)\n",
    "    axes[1, i].set_axisbelow(True)   \n",
    "    axes[1, i].legend_.remove()\n",
    "\n",
    "# Optional: globaler Titel\n",
    "fig.suptitle(\"Cluster Properties: KMeans vs GMM\", fontsize=16, y=1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plot_dataframe(df, #<-- keep it \n",
    "               x=('PCA','PC1'), # choose x-axis\n",
    "               y=(\"PCA\",'PC2'), # choose y-axis\n",
    "               z = None, # choose z-axis or set it to None for 2D plot\n",
    "               hue=('KMeans','Label') # \n",
    "              ) \n",
    "\n",
    "plot_dataframe(df, #<-- keep it \n",
    "               x=('PCA','PC1'), # choose x-axis\n",
    "               y=(\"PCA\",'PC2'), # choose y-axis\n",
    "               z = None, # choose z-axis or set it to None for 2D plot\n",
    "               hue=('GMM','Label') # \n",
    "              ) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
